{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miair\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'data/w2v_models/'\n",
    "MODEL_NAME = 'all.norm-sz100-w10-cb0-it1-min100.w2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(DIR + MODEL_NAME, binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenseProcessor(object):\n",
    "    def __init__(self, w2v_model_path, stop_list=[], tokenizer_regexp=u'[а-яА-Яa-zA-Z]+'):\n",
    "        self.w2v = self._load_w2v(w2v_model_path)\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.tokenizer = RegexpTokenizer(tokenizer_regexp)\n",
    "        self.stop_list = []\n",
    "        \n",
    "    def _load_w2v(self, w2v_model_path):\n",
    "        w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_model_path, binary=True, unicode_errors='ignore')\n",
    "        w2v.init_sims(replace=True)\n",
    "        return w2v\n",
    "    \n",
    "    def _make_bag_of_words(self, sample):\n",
    "        if type(sample) is list:\n",
    "            pass\n",
    "        elif type(sample) is str:\n",
    "            sample = sample.split()\n",
    "        else:\n",
    "            raise Exception('Sample should be string or list of words')\n",
    "        return sample\n",
    "        \n",
    "    def tokenize(self, sample):\n",
    "        '''make tokenization, return bag of words'''\n",
    "        return self.tokenizer.tokenize(sample)\n",
    "    \n",
    "    def normalize(self, sample):\n",
    "        \"\"\"make words normalization\"\"\"\n",
    "        bag_of_words = self._make_bag_of_words(sample)\n",
    "        return [self.morph.parse(word)[0].normal_form for word in bag_of_words]\n",
    "    \n",
    "    def delete_stop_words(self, sample, stop_list=[]):\n",
    "        \"\"\"delete all garbage words from sample\"\"\"\n",
    "        if not stop_list:\n",
    "            stop_list = self.stop_list\n",
    "        \n",
    "        bag_of_words = self._make_bag_of_words(sample)\n",
    "\n",
    "        for word in bag_of_words:\n",
    "            if word.lower() in stop_list:\n",
    "                bag_of_words.remove(word)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def process(self, sample, tokenize=True, normalize=True, delete_stop_words=True):\n",
    "        \n",
    "        sample = sample.lower()\n",
    "        \n",
    "        if tokenize:\n",
    "            sample = self.tokenize(sample)\n",
    "            \n",
    "        if normalize:\n",
    "            sample = self.normalize(sample)\n",
    "            \n",
    "        if delete_stop_words:\n",
    "            sample = self.delete_stop_words(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def cut_or_add(self, sample, vec_len):\n",
    "        \n",
    "        while len(sample) < vec_len:\n",
    "            sample.append(np.zeros_like(sample[0], dtype=np.float32))\n",
    "        \n",
    "        if len(sample) > vec_len:\n",
    "            sample = sample[:vec_len]\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    def convert2matrix(self, sample, vec_len=500):\n",
    "        bag_of_words = self._make_bag_of_words(sample)\n",
    "        bag_of_vectors = [self.w2v.word_vec(word) for word in bag_of_words]\n",
    "        if vec_len:\n",
    "            bag_of_vectors = self.cut_or_add(bag_of_vectors, vec_len)\n",
    "        matrix = np.array(bag_of_vectors)\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_path = DIR + MODEL_NAME\n",
    "sentence_processor = SentenseProcessor(w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['привет', 'я', 'звать', 'павел']\n",
      "['привет', 'меня', 'зовут', 'павел']\n",
      "['привет,', 'я', 'звать', 'павел']\n",
      "['привет', 'я', 'звать', 'павел', 'пока']\n"
     ]
    }
   ],
   "source": [
    "print (sentence_processor.process('Привет, меня зовут Павел'))\n",
    "print (sentence_processor.process('Привет, меня зовут Павел', normalize=False))\n",
    "print (sentence_processor.process('Привет, меня зовут Павел', tokenize=False))\n",
    "sentence_processor.stop_list = ['и', 'а', ]\n",
    "print (sentence_processor.process('Привет, меня зовут Павел и пока'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['привет', 'я', 'звать', 'павел', 'и', 'пока']\n",
      "(100, 100)\n",
      "[[-0.15173334 -0.00833917 -0.04943448 ...,  0.02435549 -0.07983094\n",
      "  -0.11920947]\n",
      " [ 0.00650954  0.02521983 -0.08965836 ...,  0.09672094  0.0080568\n",
      "  -0.09304222]\n",
      " [-0.15055948 -0.03354909 -0.05089633 ..., -0.04077737 -0.04305392\n",
      "  -0.13613304]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sent = sentence_processor.process('Привет, меня зовут Павел и пока')\n",
    "print (sent)\n",
    "matrix = sentence_processor.convert2matrix(sent, vec_len=100)\n",
    "print (matrix.shape)\n",
    "print (matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
